{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Machine Learning**\n",
    "\n",
    "It is a subset of AI focused on building systems that can learn from and make decisions based on data. Instead of being explicitly programmed, a machine learning model learns patterns from historical data and applies those patterns to new, unseen data to make predictions or decisions\n",
    "\n",
    "\n",
    "The foundation of ML is data. Models are trained on data, which can be numerical, textual, image-based, or otherwise. The quality and quantity of the data significantly impact the model's performance\n",
    "    \n",
    "    TRASH IN -> TRASH OUT\n",
    " \n",
    "<hr>\n",
    "\n",
    "## **Types of ML**\n",
    "\n",
    "### **1. Supervised Learning:**\n",
    "\n",
    "The model is trained on labeled data (data that already contains the correct output)\n",
    "\n",
    "*Example*: spam email classification (spam or not spam)\n",
    "\n",
    "*Example of models:*\n",
    "- Linear Regression - used for predicting continuous values\n",
    "- Logistic Regression - used for binary classification problems\n",
    "- Support Vector Machines (SVM) - a classification model that finds the optimal hyperplane to separate classes \n",
    "- Decision Trees - a model that splits the data into branches based on feature values to make decisions\n",
    "- Random Forest - an ensemble of decision trees used for classification or regression tasks \n",
    "- k-Nearest Neighbors (KNN) - a simple algorithm that classifies data based on the majority class of its neighbors \n",
    "- Neural Networks - deep learning models that mimic the human brain and can learn complex patterns in data \n",
    "\n",
    "\n",
    "### **2. Unsupervised Learning:**\n",
    "\n",
    "The model works with unlabeled data and tries to find hidden patterns or structures within the data\n",
    "\n",
    "*Example of models:*\n",
    "- K-Means Clustering - a clustering algorithm that partitions data into K clusters based on feature similarity\n",
    "- Hierarchical Clustering - a method that builds a hierarchy of clusters using a tree-like structure\n",
    "- Principal Component Analysis (PCA) - a technique used for dimensionality reduction, simplifying data while retaining most of its variance\n",
    "- Autoencoders - neural networks used for unsupervised feature learning and dimensionality reduction\n",
    "- t-SNE - a dimensionality reduction technique for visualizing high-dimensional data in a low-dimensional space (2D or 3D)\n",
    "\n",
    "*Example*: clustering customers into groups based on purchasing behavior\n",
    "\n",
    "### **3. Reinforcement Learning:**\n",
    "\n",
    "An agent learns by interacting with an environment and receiving rewards or penalties based on its actions\n",
    "\n",
    "*Example*: Training a robot to walk by rewarding it for staying upright and penalizing it for falling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## **The 3-step Machine Learning recipe**\n",
    "\n",
    "### **1. Define a model**\n",
    "the first step is to define the structure of the model we will use to make predictions. The model acts as a mathematical function that takes input data and produces an output (the prediction)\n",
    "\n",
    "* Model selection depends on the type of problem (classification, regression, etc.), the nature of the data, and the assumptions we can make about the data\n",
    "* The parameters of the model are often initialized randomly or with some prior knowledge\n",
    "\n",
    "### **2. Formulate a loss function (\"How bad is the model?\")**\n",
    "the next step is to define a loss function (also called a cost function or objective function). The loss function quantifies how well or poorly the model is performing by comparing the model’s predictions to the actual outcomes in the data.\n",
    "The choice of the loss function depends on the specific problem we are solving and the type of model\n",
    "\n",
    "*Common loss functions:*\n",
    "\n",
    "1. *Mean Squared Error (MSE)*\n",
    "Often used in regression problems, MSE measures the average squared difference between the predicted and actual values. The goal is to minimize the MSE to make the predictions as close as possible to the true values.\n",
    "  \n",
    "2. *Cross-Entropy Loss (Log Loss)*\n",
    "Commonly used in classification problems (especially binary and multi-class classification). It measures the difference between the true labels and the predicted probabilities, penalizing incorrect classifications more as the predicted probability moves farther from the true label\n",
    "\n",
    "3. *Log-Likelihood (Negative Log-Likelihood)*\n",
    "In probabilistic models, we use log-likelihood to measure how likely it is that the model generated the observed data. This is particularly useful in models like logistic regression or generative models. In these cases, we maximize the log-likelihood (or equivalently, minimize the negative log-likelihood) to find the parameters that best explain the data\n",
    "\n",
    "4. *Hinge Loss (for Support Vector Machines)*\n",
    "Hinge Loss is commonly used in Support Vector Machines (SVMs) for binary classification problems. It penalizes the model based on how far the predicted values are from the correct classification margin.\n",
    "The hinge loss encourages the model to correctly classify examples and maintain a margin between the classes\n",
    "\n",
    "#### **Connection to estimation methods**\n",
    "Some commonly used loss functions directly correspond to statistical estimation methods:\n",
    "\n",
    "* Ordinary Least Squares (OLS): Used in linear regression, this corresponds to minimizing the Mean Squared Error (MSE). OLS finds the line (or hyperplane) that minimizes the squared differences between predicted and actual values\n",
    "* Maximum Likelihood Estimation (MLE): A general method used in probabilistic models. For example, in logistic regression, minimizing the Cross-Entropy Loss is equivalent to using MLE. MLE chooses parameters that maximize the likelihood of observing the given data\n",
    "\n",
    "\n",
    "### **3. Minimize the loss**\n",
    "Once we have defined the model and the loss function, the next step is to minimize the loss in order to find the optimal parameters for the model. This is typically done using optimization algorithms like Gradient Descent\n",
    "\n",
    "\n",
    "#### **Gradient descent**\n",
    "The main idea behind gradient descent is to adjust the model’s parameters step by step, moving in the direction that reduces the loss. Gradient descent computes the gradient (or slope) of the loss function with respect to each parameter (weight), and uses this to adjust the parameter in the direction that minimizes the loss\n",
    "\n",
    "\n",
    "**Key Steps in Gradient Descent**\n",
    "* calculate the gradient - determine how much the loss function changes with respect to each parameter\n",
    "* update the parameters - adjust the model’s parameters by moving in the direction opposite to the gradient (this is how you minimize the loss)\n",
    "* repeat - continue this process iteratively, taking small steps toward the minimum of the loss function until the model’s performance stabilizes (i.e. the loss stops decreasing significantly)\n",
    "\n",
    "**Types of Gradient Descent**\n",
    "1. *Batch Gradient Descent*\n",
    "* Calculates the gradient using the entire dataset\n",
    "* This is very accurate, but can be slow and computationally expensive, especially for large datasets\n",
    "* Pros: Guaranteed to find the optimal solution if the dataset is small enough\n",
    "* Cons: Slow for large datasets because it needs to process all data points before updating the model.\n",
    "2. *Stochastic Gradient Descent (SGD)*\n",
    "* Instead of calculating the gradient using the entire dataset, SGD uses a single data point at a time to update the parameters\n",
    "* This makes it faster but more \"noisy\" because the updates can be more erratic.\n",
    "* Pros: Faster and can handle large datasets efficiently\n",
    "* Cons: It may not converge as smoothly, but can still eventually reach a good solution with enough iterations\n",
    "3. *Mini-batch Gradient Descent*\n",
    "* A compromise between batch gradient descent and SGD. It uses a small subset (mini-batch) of the dataset to compute the gradient and update the parameters\n",
    "* This approach is commonly used because it balances speed and accuracy\n",
    "* Pros: Faster than batch gradient descent, more stable than SGD\n",
    "* Cons: Requires tuning to find the optimal mini-batch size\n",
    "\n",
    "**Learning Rate** \n",
    "The learning rate determines the size of the steps taken in gradient descent. It's a crucial hyperparameter that impacts how quickly or slowly the model converges. Finding the right learning rate is essential to training a model efficiently:\n",
    "\n",
    "* if the learning rate is too high, the model might take oversized steps and miss the minimum. This can make the model unstable and prevent it from converging\n",
    "* if the learning rate is too low, the model will take tiny steps, resulting in slow convergence and requiring more iterations to reach the minimum\n",
    "\n",
    "**Convergence and stopping criteria**\n",
    "The goal of optimization is to find the minimum loss (or as close to it as possible), and once that is achieved, we stop. Convergence happens when the model's parameters stabilize and the loss function doesn't decrease significantly anymore\n",
    "\n",
    "Common stopping criteria:\n",
    "\n",
    "* fixed number of iterations - stop after a certain number of updates\n",
    "* tolerance - stop when the improvement in loss is smaller than a defined threshold (indicating that the model is not learning much more)\n",
    "* validation performance - stop when performance on the validation set starts to degrade (avoiding overfitting)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## **Bias-Variance tradeoff**\n",
    "Bias and variance refer to two main sources of error in machine learning models\n",
    "\n",
    "#### **Model bias**\n",
    "\n",
    "High bias means the model has a simplistic structure and fails to capture the complexity of the data. This can lead to **underfitting**, where the model cannot fit the training data well, resulting in poor predictions both on training and test data\n",
    "\n",
    "#### **Variance**\n",
    "\n",
    "High variance means the model is too complex and overly dependent on the training data, which can lead to **overfitting**. In this case, the model performs well on the training data but struggles to generalize to unseen data\n",
    "\n",
    "The goal is to find the balance between bias and variance. Too low bias leads to high variance and vice versa\n",
    "\n",
    "\n",
    "#### **Ways to tackle overfitting**\n",
    "1. Get more data\n",
    "2. Reduce model complexity. For example, use a template with fewer parameters. This makes it harder for the model to memorize stuff\n",
    "3. Use regularization (restrict the values the parameters can take)\n",
    "    * L1 Regularization (Lasso) - adds a penalty to the loss function based on the absolute values of model weights\n",
    "    * L2 Regularization (Ridge) - adds a penalty based on the squared values of the model weights. L2 regularization helps control the size of the weights and prevents overfitting\n",
    "4. Cross-validation. Using cross-validation techniques allows for better performance evaluation on different subsets of the training data, helping to avoid overfitting\n",
    "5. For some models, like decision trees, pruning removes excess \"noisy\" branches that are tuned to specific cases in the training data, preventing overfitting\n",
    "6. During training neural networks you can early stop, this technique stops training when performance on the validation set stops improving\n",
    "7. Enriching the training data by applying transformations such as image rotations, scaling, or adding noise. This helps the model learn more generalizable features\n",
    "\n",
    "#### **Ways to tackle underfitting**\n",
    "1. Increase model complexity (e.g. moving from linear regression to a neural network or decision tree). The more complex the model, the better it can capture non-linear relationships in the data\n",
    "2. Decrease regularization. Too strong regularization can lead to underfitting by restricting the model’s ability to learn Regularization parameters (e.g., L1 and L2 coefficients) can be reduced to allow the model more flexibility\n",
    "3. Increase the number of features. If the model has too few features, it may not be able to capture important patterns. Adding new features or applying feature extraction techniques (e.g. PCA) can provide more information to the model\n",
    "4. Increase the number of iterations. For iterative learning models increasing the number of training epochs or iterations can help the model learn more complex patterns\n",
    "5. Sometimes switching to a different algorithm can help\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## **The No Free Lunch Theorem (NFLT)**\n",
    "\n",
    "The No Free Lunch Theorem is a principle in optimization and machine learning that states that no single algorithm is universally best for all problems. In practice, this means there is no algorithm that will always outperform others for all possible tasks. Every algorithm has its limitations and works better or worse depending on the problem at hand\n",
    "\n",
    "#### **Implications of NFLT in ML**\n",
    "* Algorithm selection depends on the problem. Which model performs best in a given situation depends on the nature of the data and the problem you're trying to solve. For example, linear regression may work well for simple data, but may struggle with more complex, non-linear data.\n",
    "* Experimenting with different models, because of this, when solving ML problems, it's important to test multiple algorithms to identify which one works best for the specific problem at hand. There are various techniques and models that perform differently on different kinds of data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# **Common mistakes**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
